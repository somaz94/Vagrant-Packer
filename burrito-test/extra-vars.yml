################
### Tacoplay ###
################

## TACO storage backend (ceph|netapp_nfs|unity_fc|hitachi_fc)
taco_storage_backend: "ceph"

## TACO applications will be deployed on kubernetes. (openstack|lma)
taco_apps: ["openstack"]

## TACO for offline installation
container_registry_enabled: true
pip_repo_url: "control01:8089"
pkg_repo_url: "control01:8090"
k8s_binary_repo_url: "control01:8090"
ceph_repo_url: "control01:8090"
storage_vendor_url: "control01:8090"


##################
##### Docker #####
##################

docker_insecure: ['192.168.20.10:5000','control01:5000','control02:5000','control03:5000']


##################
### Keepalived ###
##################

keepalived_enabled: true
keepalived:
  - vip_interface: eth1
    vip: 192.168.20.10


##################
### Kubernetes ###
##################

## pod replicas
taco_replicas: 3

## calico mtu
calico_mtu: 1500


#################
### OpenStack ###
#################

## openotack mtu
openstack_mtu: 1500

## NVIDIA vGPU types
vgpu_types: null

## live_migration_interface
live_migration_interface: eth1

## neutron ml2 plugin (linuxbridge|ovs)
neutron_ml2_plugin: linuxbridge

# **linuxbridge** provider uses flat
lb_provider:
  - "external:eth3"

# **ovs** provider network should use vlan for multi-provider
ovs_provider:
  - name: external
    bridge: br-ex
    iface: eth3
    vlan_ranges: "150:151"

## tunnel is a overlay network interface
tunnel: eth2

## hypervisor my_ip interface in nova.conf
hypervisor_mgmt_interface: eth1

## Neutron BGP Dynamic Routing Agent (set it true to enable it)
bgp_dragent: false

## NoVNC bind iface
novnc_listen_iface: eth1


##################
### NetApp NFS ###
##################
# If taco_storage_backend is "netapp_nfs", set it below.

## nfs provisioner
nfs_provisioner:
  nfs_server_ip: 192.168.140.19
  nfs_share_path: /taco2_nfs_provisioner

## nfs glance
glance_store:
  nfs_server_ip: 192.168.140.19
  nfs_share_path: /taco2_glance

## nfs backup
backup_store:
  nfs_server_ip: 192.168.140.19
  nfs_share_path: /taco2_backup

## netapp cinder
netapp_nfs:
  - name: nfs1
    server_ip: 192.168.140.19
    login_id: vsadmin
    password: YWdpbEVeQDY2Cg==
    vserver_name: svm01
    nas_share_path: /taco2_cinder
  - name: nfs2
    server_ip: 192.168.140.19
    login_id: vsadmin
    password: YWdpbEVeQDY2Cg==
    vserver_name: svm01
    nas_share_path: /taco2_cinder2


##################
#### Unity FC ####
##################
# If taco_storage_backend is "unity_fc", set it below.

## unity cinder
unity_fc:
  - name: unity-fc
    san_ip: 192.168.120.20
    san_login: admin
    san_password: UGFzc3dvcmQxMjMhCg==
    unity_io_ports: "spa_iom_1_fc0, spb_iom_1_fc0, spa_iom_1_fc1, spb_iom_1_fc1"
    unity_storage_pool_names: p0


####################
#### Hitachi FC ####
####################
# If taco_storage_backend is "hitachi_fc", set it below.

## hitachi cinder
hitachi_fc:
  - name: hitachi-fc
    hitachi_storage_id: "934000611322"
    hitachi_pool: 0
    hitachi_rest_api_ip: 192.168.120.91
    hitachi_rest_user: maintenance
    hitachi_rest_password: cmFpZC1tYWludGVuYW5jZQo=
    hitachi_target_ports: "CL1-A,CL2-A"
    hitachi_compute_target_ports: "CL3-A,CL4-A,CL1-B,CL2-B,CL1-A,CL2-A"


########################
#### PureStorage FC ####
########################
# If taco_storage_backend is "pure_fc", set it below.

## pure cinder
pure_fc:
  - name: pure-fc
    san_ip: 192.168.120.22
    pure_api_token: "8784b811-4f92-49a8-1ea7-18e9c4e36720"


############
### Ceph ###
############
# If taco_storage_backend is "ceph", set it below.

## ceph monitor
monitor_interface: eth4
public_network: 192.168.50.0/24
cluster_network: 192.168.60.0/24

## ceph rados-gateway
radosgw_frontend_type: civetweb
radosgw_civetweb_port: 7480
radosgw_interface: eth0
rgw_create_pools:
  "{{ rgw_zone }}.rgw.buckets.data":
    pg_num: 16
  "{{ rgw_zone }}.rgw.buckets.index":
    pg_num: 16

## ceph config
ceph_conf_overrides:
      global:
        mon_allow_pool_delete: true
        osd_pool_default_size: 1
        osd_pool_default_min_size: 0
        # for only one node.
        #osd_crush_chooseleaf_type: 0

## ceph osd
osd_objectstore: bluestore
lvm_volumes:
  - data: /dev/vdb
  - data: /dev/vdc
  - data: /dev/vdd
  
## ceph openstack pools
openstack_config: true
kube_pool:
  name: "kube"
  pg_num: 32
  pgp_num: 32
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
  application: "rbd"
openstack_glance_pool:
  name: "images"
  pg_num: 64
  pgp_num: 64
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
openstack_cinder_pool:
  name: "volumes"
  pg_num: 64
  pgp_num: 64
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
openstack_cinder_backup_pool:
  name: "backups"
  pg_num: 8
  pgp_num: 8
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""
openstack_nova_vms_pool:
  name: "vms"
  pg_num: 8
  pgp_num: 8
  rule_name: "replicated_rule"
  type: 1
  erasure_profile: ""
  expected_num_objects: ""

openstack_pools:
  - "{{ kube_pool }}"
  - "{{ openstack_glance_pool }}"
  - "{{ openstack_cinder_pool }}"
  - "{{ openstack_cinder_backup_pool }}"
  - "{{ openstack_nova_vms_pool }}"
